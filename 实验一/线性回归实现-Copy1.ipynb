{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Tensor和autograd来实现一个线性回归，具体的步骤有：\n",
    "* 生成和读取数据集\n",
    "* 构建模型\n",
    "* 初始化模型参数\n",
    "* 定义损失函数和优化算法\n",
    "* 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入本次实验所需的包或模块，其中matplotlib包可用于作图，用来显示生成的数据的二维图。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本实验先构造一个简单训练数据集，通过这个数据集可以直观的比较模型训练出来的参数和真实的模型参数的区别。设训练数据集样本数为1000，输入个数（特征数）为2，给定随机生成的批量样本特征X∈R^1000×2。这里使用线性回归模型的真实权重w=[2，-3.4].T和偏差b=4.2，以及一个随机噪声项e来生成标签。  \n",
    "  \n",
    "### y=xw.T+b+e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其中，噪声项e是服从均值0、标准差为0.01的正态分布。噪声代表了数据中无意义的干扰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 500\n",
    "num_examples = 10000\n",
    "true_w = 0.0056 \n",
    "true_b = 0.028\n",
    "features = torch.tensor(np.random.normal(0,1,(num_examples,num_inputs)),dtype=torch.float)\n",
    "labels = true_w*features.sum(dim=1)+true_b\n",
    "labels += torch.tensor(np.random.normal(0,0.001,size=labels.size()),dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x18ef5e1fcf8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3.5,2.5))\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-10,20)\n",
    "plt.scatter(features[:,1].numpy(),labels.numpy(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在模型训练的时候，需要遍历数据集并不断读取下批量的数据样本。这里本实验定义一个函数`data_iter()` 它每次返回batch_size（批量大小）个随机样本的特征与标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 500\n",
    "def data_iter(batch_size,features,labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices) #样本的读取顺序是随机的\n",
    "    for i in range(0,num_examples,batch_size):\n",
    "#         print(i)\n",
    "        j = torch.LongTensor(indices[i:min(i+batch_size,num_examples)])# 最后一次可能不足一个batch\n",
    "        yield features.index_select(0,j),labels.index_select(0,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在构建模型之前，需要将权重和偏置初始化。本实验将权重初始化成均值为0、标准差为0.01的正态随机数，偏置初始化为0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(np.random.normal(0,0.01,(num_inputs,1)),dtype=torch.float32)\n",
    "b = torch.zeros(1,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在后面的模型训练中，需要对这些参数求梯度来迭代参数的值，  \n",
    "### 因此要设置requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面使用mm()函数做矩阵乘法，来实现线性回归的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w,b):\n",
    "    return torch.mm(X,w)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数和优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本实验使用平方损失来定义线性回归的损失函数。在实现中，我们需要把真实值y变形成预测值y_hat形状。以下的函数返回的结果和y_hat的形状相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat,y):\n",
    "    return (y_hat-y.view(y_hat.size()))**2/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下的sgd函数实现了小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得到的是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params,lr,batch_size):\n",
    "    for param in params:\n",
    "        param.data -= lr*param.grad/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在训练过程中，模型将会多次迭代更新参数。在每次迭代中，根据当前读取的小批量数据样本（特征x和标签y），通过调用反向函数backward计算小批量随机梯度，并调用优化算法sgd迭代模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.031239\n",
      "epoch2,loss0.025239\n",
      "epoch3,loss0.020433\n",
      "epoch4,loss0.016578\n",
      "epoch5,loss0.013477\n",
      "epoch6,loss0.010980\n",
      "epoch7,loss0.008963\n",
      "epoch8,loss0.007332\n",
      "epoch9,loss0.006010\n",
      "epoch10,loss0.004936\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "batch_size = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l = loss(net(X,w,b),y).sum()\n",
    "        l.backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    train_l = loss(net(features,w,b),labels)\n",
    "    print('epoch%d,loss%f'%(epoch+1,train_l.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] \n",
      " tensor([[ 1.9982],\n",
      "        [-3.3937]], requires_grad=True)\n",
      "4.2 \n",
      " tensor([4.1927], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(true_w,'\\n',w)\n",
    "print(true_b,'\\n',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用torch.nn实现线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PyTorch提供了data库来读取数据。由于data常用作变量名，这里将导入的data模块用Data代替。对前面的读取数据部分可以使用data库来处理。在每一次迭代中，使用Data随机读取包含10个数据样本的小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "batch_size = 100\n",
    "lr = 0.3\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features,labels)\n",
    "\n",
    "# 把dataset放入DataLoader\n",
    "data_iter = Data.DataLoader(\n",
    "            dataset = dataset,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 构建模型的过程中，最常见的方法就是继承nn.Module，然后构建自己的网络。一个nn.Module实例需要包含一些层以及返回输出的前向传播方法。下面利用nn.Module构建一个线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.linear = nn.Linear(n_feature,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "    \n",
    "net = LinearNet(num_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在使用定义的模型net之前，需要对模型中的一些参数进行初始化。Pytorch在init模块中提供了许多初始化参数的方法。我们可以调用init.normal模块通过正态分布对线性回归中的权重和偏差进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net.linear.weight,mean=0,std=0.01)\n",
    "init.constant_(net.linear.bias,val=0) #也可以直接修改bias的data：net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pytorch在torch.nn中提供了各种损失函数，这些损失函数实现为nn.Module的子类，可以将这些损失函数作为一种特殊的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pytorch在torch.optim模块中提供了诸如SGD，Adam和RMSProop等优化算法。本例将使用小批量随机梯度下降算法进行优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.03)\n",
    "\n",
    "# 可以为不同的子网络设置不同学习率\n",
    "# optimizer = optim.SGD([\n",
    "#     # 如果不指定学习率，则会默认使用最外层学习率\n",
    "#     {'params': net.subnet1.parameters()}\n",
    "#     {'params': net.subnet2.parameters(),'lr':0.01}\n",
    "# ],lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 训练模型时，可以调用optim中的step()函数来迭代模型参数。按照小批量随机梯度下降的定义，在step()函数中指定批量大小，从而对批量中的样本梯度求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss: 0.000083\n",
      "epoch 2,loss: 0.000090\n",
      "epoch 3,loss: 0.000082\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for X,y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output,y.view(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d,loss: %f'%(epoch,l.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测及评价（分类问题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对于分类问题，给定任一样本特征，模型可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别(标签)一致，说明这次预测时正确的。我们使用准确率(accuracy)来评价模型的表现。它等于正确实现预测数量与总预测数量之比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 下面我们给出准确率计算函数的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y):\n",
    "    return (y_hat.argmax(dim=1)==y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 评价模型net在数据集data_itear上的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum,n = 0.0,0\n",
    "    for X,y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1)==y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum/n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
